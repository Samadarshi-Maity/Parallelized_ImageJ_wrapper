{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea089b5",
   "metadata": {},
   "source": [
    "# Check how the particles are moving in the track \n",
    "\n",
    "1. Import the data correctly\n",
    "2. Make it a habit to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d2268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012778f9",
   "metadata": {
    "code_folding": [
     0,
     6,
     46,
     66,
     87,
     175,
     196
    ]
   },
   "outputs": [],
   "source": [
    "# define some useful functions \n",
    "\n",
    "# Here I will place will the functions that Will be necessary for the ETL process. \n",
    "# ~~~~~~~ trouble shoot what the problem is with functions.py\n",
    "\n",
    "# extract the image paths \n",
    "def extract_ImagePaths(expPath, nth_image, voltage):\n",
    "    '''\n",
    "    Function extracts the nth image. \n",
    "    params:\n",
    "    expPath  : list of folder names that contain all the experiment, the folder before (\"/..._files\") folders\n",
    "    nth_image: nth image whose path needs to be used.  \n",
    "    returns: \n",
    "    imagePathExtract = list of two Image Paths for each experiment \n",
    "    '''\n",
    "    #particle count in each experiment\n",
    "    count_list = [int(x.split('\\\\')[-1].split('_')[0]) for i,x in enumerate(expPath)]\n",
    "\n",
    "    # initialise and sort the data frame to finally get the correct order\n",
    "    filename_data = pd.DataFrame()\n",
    "    filename_data['paths'] = expPath\n",
    "    filename_data['number'] = count_list\n",
    "\n",
    "    # sort the filenames in the correct order\n",
    "    filename_data = filename_data.sort_values(by = 'number')\n",
    "    \n",
    "    str1 = r'\\*';\n",
    "    str3 = r'*_files';\n",
    "    str2 = str(voltage);\n",
    "    A = str1 + str2 + str3;\n",
    "    \n",
    "    imagePathExtract = []\n",
    "    for i,filePath in enumerate(filename_data['paths']):\n",
    "\n",
    "        # extract all the experimentPaths\n",
    "        experimentPath = glob.glob(filePath+A)\n",
    "\n",
    "        # extract the images paths from the experiment Paths\n",
    "        for experiment in experimentPath:\n",
    "            # extract the image filenames in each experiment\n",
    "            imagePath = glob.glob(experiment+'\\Images\\*.tif')\n",
    "            # save nth image file names from the list \n",
    "            imagePathExtract.append([imagePath[nth_image]])\n",
    "    return imagePathExtract \n",
    "\n",
    "# extract the velocity arrays from the .mat files\n",
    "def Vel_data_ETL(path):\n",
    "    '''\n",
    "    This is a function to extract the data from VelocitiesF.mat/VelocitiesNF.mat/ \n",
    "    '''\n",
    "    # create a dataframe to store the data extracted from .mat file \n",
    "    XYFPVxVy = pd.DataFrame()\n",
    "    \n",
    "    # extract the data from the .mat file \n",
    "    f = h5py.File(path, 'r')\n",
    "    MATLAB_data = {}\n",
    "    for k, v in f.items():\n",
    "        MATLAB_data[k] = np.array(v)\n",
    "\n",
    "    for i,column_name in enumerate(['X','Y','F','P','Vx','Vy']):   # I and Rg is droppped here \n",
    "        XYFPVxVy[column_name]= (MATLAB_data['XYFPVxVy'])[i];\n",
    "    del(MATLAB_data) \n",
    "    f.close()\n",
    "    return XYFPVxVy\n",
    "\n",
    "# extract the densiy arrays from the .mat files \n",
    "def Den_data_ETL(path):\n",
    "    '''\n",
    "    This is a function to extract the data from Positions_F.mat/Positions_NF.mat\n",
    "    '''\n",
    "    # create a dataframe to store the data extracted from .mat file \n",
    "    XYF = pd.DataFrame()\n",
    "    \n",
    "    # extract the data from the .mat file \n",
    "    f = h5py.File(path, 'r')\n",
    "    MATLAB_data = {}\n",
    "    for k, v in f.items():\n",
    "        MATLAB_data[k] = np.array(v)\n",
    "\n",
    "    for i,column_name in enumerate(['X','Y','F']): # don't care about 'I' and 'Rg'\n",
    "        XYF[column_name]= (MATLAB_data['XYF'])[i];\n",
    "    del(MATLAB_data) \n",
    "    \n",
    "    f.close()\n",
    "    return XYF\n",
    "\n",
    "# bring the density data and the velocity data to compute the radial binned averages \n",
    "def Transform_vel_pos_matrix(radius, dA, edges, centres_vel, centres_den, vel_path):\n",
    "    '''\n",
    "    This function computes single \"binData\" from separate entry of vel_vectors and Density data.\n",
    "    IMP: Here, I am making some modifications to use the XY from the velocity data itself \n",
    "    PARAMS: \n",
    "        centres_vel: centre of the images used for velocity computations\n",
    "        centres_den: centre of the images used for density computations\n",
    "        den_path:    path of the density matrix (.mat)\n",
    "        vel_path:    path of e velocity matrix  (.mat)\n",
    "        bins:        number of bins\n",
    "        conf_rad:    radius of the circular confinment in px.\n",
    "    \n",
    "    RETURNS:\n",
    "        bindata: data with \"R, theta, V0, V_phi, V_r, P_phi , P_r, Pi,rho, Ar_frac\"\"\n",
    "    '''\n",
    "    \n",
    "    # Extract the matrix for the velocity      \n",
    "    vel_vectors = Vel_data_ETL(vel_path)\n",
    "\n",
    "    # centre of the velocity images  \n",
    "    centres_vel = np.array(centres_vel)\n",
    "\n",
    "    # centre of the density images\n",
    "    centres_den = np.array(centres_den)  \n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Radial profiles ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ velocity vector computations ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # compute the Radial distances.\n",
    "    vel_vectors['R'] = np.sqrt((vel_vectors['X'] -centres_vel[0]) **2  + (vel_vectors['Y'] - centres_vel[1])**2)\n",
    "    # compute the theta using the relatve position of the particle to the vortex centre \n",
    "    vel_vectors['theta'] = np.arctan2((vel_vectors['Y']-centres_vel[1]), (vel_vectors['X']-centres_vel[0]))\n",
    "    \n",
    "    # measure the total number of frames\n",
    "    total_frames = max(vel_vectors['F'])\n",
    "    # clear the memory of the X and Y values \n",
    "    vel_vectors.drop(columns = ['X','Y','P'], inplace=True)\n",
    "    \n",
    "    # compute the propulsion speed\n",
    "    vel_vectors['V0'] = np.sqrt(vel_vectors['Vx']**2 +vel_vectors['Vy']**2)\n",
    "    # compute the radial and the azimuthal velocities unit vectors \n",
    "    vel_vectors['P_phi'] = (-vel_vectors['Vx']*np.sin(vel_vectors['theta']) + vel_vectors['Vy']*np.cos(vel_vectors['theta']))/vel_vectors['V0']\n",
    "    vel_vectors['P_r'] = (vel_vectors['Vx']*np.cos(vel_vectors['theta']) + vel_vectors['Vy']*np.sin(vel_vectors['theta']))/vel_vectors['V0']\n",
    "    \n",
    "    # nematic order parameters computations\n",
    "    vel_vectors['P_phi_P_phi'] = vel_vectors['P_phi']*vel_vectors['P_phi']\n",
    "    vel_vectors['P_r_P_r']     = vel_vectors['P_r']*vel_vectors['P_r']\n",
    "    vel_vectors['P_r_P_phi']   = vel_vectors['P_phi']*vel_vectors['P_r']\n",
    "    \n",
    "    # clear the memory of the Vx and Vy , Vphi and Vr values \n",
    "    #vel_vectors.drop(columns = ['V_phi', 'V_r'], inplace=True)\n",
    "    \n",
    "    # create another column with the BIN IDs\n",
    "    vel_vectors['binID'] = np.digitize(vel_vectors['R'], edges, right=True)\n",
    "    \n",
    "\n",
    "    # develop and intantiate the bins [do this too avoid referencing complications]\n",
    "    #binData = pd.DataFrame()\n",
    "    # create  the 'binData' from the velocity vector data. \n",
    "    binData = vel_vectors.groupby(by=['binID']).mean()\n",
    "    \n",
    "    # Compute the mean polarisation in each bin \n",
    "    binData['Pi'] = np.sqrt((binData['P_r'])**2 + (binData['P_phi'])**2)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ density computations ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    # extract the matrix for the density\n",
    "    XYF = pd.DataFrame()\n",
    "    XYF['R'] = vel_vectors['R']\n",
    "    XYF['F'] = vel_vectors['F']\n",
    "    XYF['binID'] = np.digitize(XYF['R'], edges, right=True)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ add the rho to the bin Data ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # compute the count sizes\n",
    "    binData['rho'] = XYF.groupby(by=['binID']).size()\n",
    "\n",
    "    # cnsider only the first n bins that you require...\n",
    "    binData = binData[:len(dA)]\n",
    "    \n",
    "    binData['rho'] = binData['rho']/(total_frames*dA)\n",
    "    \n",
    "    # compute the average density of experiment\n",
    "    average_density = len(XYF)/total_frames\n",
    "    # compute the Area Fractions\n",
    "    binData['Ar_frac'] = binData['rho']*(np.pi*radius**2) \n",
    "    # return the stuff\n",
    "    return binData, average_density, vel_vectors\n",
    "\n",
    "# build the final path to the density and velocity files \n",
    "def part_type_path_list(particle, den_expPath, velocity_expPath):\n",
    "    '''\n",
    "    This function builds the path to the density and velocity files \n",
    "    '''\n",
    "    # choose the particle type\n",
    "    if particle == 'F':\n",
    "        denPathExt = r'\\**\\Analysis\\1_Positions\\Positions_F.mat'\n",
    "        velPathExt = r'\\**\\Analysis\\3_Velocities\\VelocitiesF.mat'\n",
    "        particle_rad = 5 #um\n",
    "    elif particle =='NF':\n",
    "        denPathExt = r'\\**\\Analysis\\1_Positions\\Positions_NF.mat'    \n",
    "        velPathExt = r'\\**\\Analysis\\3_Velocities\\VelocitiesNF.mat'\n",
    "        particle_rad = 3.5 #um\n",
    "        \n",
    "    # Extract the matrix for the velocity      \n",
    "    VelFilePath = glob.glob(velocity_expPath+velPathExt)\n",
    "\n",
    "    # extract the matrix for the density\n",
    "    DenFilePath = glob.glob(den_expPath+denPathExt)\n",
    "    return DenFilePath, VelFilePath\n",
    "\n",
    "def part_type_path_list_fragmented(fragment, particle, den_expPath, velocity_expPath):\n",
    "    '''\n",
    "    This function builds the path to the density and velocity files \n",
    "    Here, we pick up the fragments of the same confiment (for 7um)\n",
    "    becoz, the along r within the same ROI, i could not capture the entire radial distribution\n",
    "    '''\n",
    "    if (fragment == 'A'):\n",
    "    \n",
    "        # choose the particle type\n",
    "        if particle == 'F':\n",
    "            denPathExt = r'\\*A_files\\Analysis\\1_Positions\\Positions_F.mat'\n",
    "            velPathExt = r'\\*A_files\\Analysis\\3_Velocities\\VelocitiesF.mat'\n",
    "            particle_rad = 5 #um\n",
    "        elif particle =='NF':\n",
    "            denPathExt = r'\\*A_files\\Analysis\\1_Positions\\Positions_NF.mat'    \n",
    "            velPathExt = r'\\*A_files\\Analysis\\3_Velocities\\VelocitiesNF.mat'\n",
    "            particle_rad = 3.5 #um\n",
    "    \n",
    "    else:\n",
    "        # choose the particle type\n",
    "        if particle == 'F':\n",
    "            denPathExt = r'\\*B_files\\Analysis\\1_Positions\\Positions_F.mat'\n",
    "            velPathExt = r'\\*B_files\\Analysis\\3_Velocities\\VelocitiesF.mat'\n",
    "            particle_rad = 5 #um\n",
    "        elif particle =='NF':\n",
    "            denPathExt = r'\\*B_files\\Analysis\\1_Positions\\Positions_NF.mat'    \n",
    "            velPathExt = r'\\*B_files\\Analysis\\3_Velocities\\VelocitiesNF.mat'\n",
    "            particle_rad = 3.5 #um\n",
    "    \n",
    "    # Extract the matrix for the velocity      \n",
    "    VelFilePath = glob.glob(velocity_expPath+velPathExt)\n",
    "\n",
    "    # extract the matrix for the density\n",
    "    DenFilePath = glob.glob(den_expPath+denPathExt)\n",
    "    return DenFilePath, VelFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e425647",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 18\n",
    "confinement_radius = 1025 #px\n",
    "vel_centre = [1176, 1028]\n",
    "den_centre = [1176, 1028]\n",
    "edges = np.linspace(1,confinement_radius,(bins+1));\n",
    "\n",
    "# define the edges and the elementary area and the central value of R\n",
    "dA =  np.pi*(edges[1:]**2 - edges[:-1]**2)\n",
    "\n",
    "#~~~~~~~~~~~~ Store the binData tables as lists ~~~~~~~~~~~~~~~\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~ compute for 'NF' particles ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ declare the master paths ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "path = r'D:\\Ring formation in flocks\\2_files\\Analysis\\3_Velocities\\VelocitiesNF.mat'\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "particle = 'NF'\n",
    "radius = 3.5 #um \n",
    "# obtain the final bin matrix\n",
    "[bindata_NF, avg_density_NF,vel_data_NF] = Transform_vel_pos_matrix(radius,dA, edges, vel_centre, den_centre, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b97ab50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.1405722457806884"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(vel_data_NF['theta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c117eef",
   "metadata": {},
   "source": [
    "# Do the following things:\n",
    "1. extract only a part of the confinement which has a clear track formation.\n",
    "2.\n",
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24a009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
